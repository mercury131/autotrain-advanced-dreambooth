# %% [markdown]
# <a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_lora_training-sdxl.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
#
# # SDXL DreamBooth LoRA Training
#
# This notebook trains a LoRA adapter for Stable Diffusion XL (SDXL) using the DreamBooth technique. LoRA drastically reduces the number of trainable parameters, making training faster and the resulting weights much smaller (~100MB).
#
# **Before running:**
# 1. Upload your training images to the `/images` folder in Colab
# 2. Choose a unique identifier token (like "sks" or a made-up word) that doesn't appear in normal dictionaries
# 3. Adjust hyperparameters as needed for your specific use case

# %% [code]
# Install required dependencies
!pip install -q --upgrade diffusers[torch] transformers accelerate datasets
!pip install -q --upgrade bitsandbytes safetensors xformers

# %% [code]
import os
import torch
from huggingface_hub import notebook_login

# Login to Hugging Face Hub (optional - for pushing to hub)
# notebook_login()

# %% [markdown]
# ## Configuration
#
# Customize these parameters for your training:

# %% [code]
# @title Training Configuration
# Project settings
project_name = "converted_image"  # @param {type:"string"}
output_dir = f"/content/{project_name}"

# Model settings
model_name = "stabilityai/stable-diffusion-xl-base-1.0"  # @param ["stabilityai/stable-diffusion-xl-base-1.0"]

# Training data
instance_data_dir = "/content/images"  # @param {type:"string"}
instance_prompt = "a photo of a sks professional"  # @param {type:"string"}
# Use a unique token like "sks" that doesn't appear in normal text

# Training parameters
resolution = 1024  # @param {type:"integer"}
batch_size = 1  # @param {type:"integer"}
gradient_accumulation_steps = 4  # @param {type:"integer"}
learning_rate = 1e-4  # @param {type:"number"}
num_train_steps = 500  # @param {type:"integer"}
validation_steps = 100  # @param {type:"integer"}

# Optimization settings
use_8bit_adam = True  # @param {type:"boolean"}
use_xformers = True  # @param {type:"boolean"}
use_fp16 = True  # @param {type:"boolean"}
gradient_checkpointing = True  # @param {type:"boolean"}
train_text_encoder = False  # @param {type:"boolean"}

# Validation settings
validation_prompt = "a sks professional portrait"  # @param {type:"string"}
num_validation_images = 4  # @param {type:"integer"}

# Push to Hub settings (optional)
push_to_hub = False  # @param {type:"boolean"}
hub_model_id = "your-username/your-model-name"  # @param {type:"string"}

# Create output directory
os.makedirs(output_dir, exist_ok=True)

print(f"Training configuration:")
print(f"- Model: {model_name}")
print(f"- Instance prompt: {instance_prompt}")
print(f"- Output directory: {output_dir}")
print(f"- Resolution: {resolution}")
print(f"- Batch size: {batch_size}")
print(f"- Learning rate: {learning_rate}")
print(f"- Training steps: {num_train_steps}")
print(f"- Gradient accumulation: {gradient_accumulation_steps}")
print(f"- Text encoder training: {train_text_encoder}")

# %% [markdown]
# ## Prepare Training Data
#
# Upload your images to the `/content/images` folder. You can do this manually through the Colab file browser or use the cell below:

# %% [code]
# Create images directory if it doesn't exist
os.makedirs(instance_data_dir, exist_ok=True)

print(f"Please upload your training images to: {instance_data_dir}")
print("You can upload them manually through the Colab file browser (folder icon on left sidebar) or run the next cell to upload files.")

# %% [code]
# @title Upload Images (Optional - Run if you want to upload directly)
from google.colab import files
import shutil

print("Upload your training images (minimum 3-5 images recommended):")
uploaded = files.upload()

# Move uploaded files to the images directory
for filename in uploaded.keys():
    if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.webp', '.bmp')):
        shutil.move(filename, os.path.join(instance_data_dir, filename))
        print(f"Moved {filename} to {instance_data_dir}")

# Verify images were uploaded
image_files = [f for f in os.listdir(instance_data_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.webp', '.bmp'))]
print(f"\nFound {len(image_files)} images in {instance_data_dir}:")
for img in image_files:
    print(f"- {img}")

# %% [markdown]
# ## Setup Training Environment
#
# Configure the training environment with proper optimizations for Colab's T4 GPU:

# %% [code]
from accelerate.utils import write_basic_config

# Write accelerate config for mixed precision training
write_basic_config(mixed_precision="fp16" if use_fp16 else "no")

print("Accelerate config created successfully!")

# %% [markdown]
# ## Start Training
#
# This will train the LoRA adapter. Training time depends on the number of steps and your GPU.

# %% [code]
import subprocess
import json

# Create training command
command = [
    "accelerate", "launch", "train_dreambooth_lora_sdxl.py",
    "--pretrained_model_name_or_path", model_name,
    "--instance_data_dir", instance_data_dir,
    "--output_dir", output_dir,
    "--instance_prompt", instance_prompt,
    "--resolution", str(resolution),
    "--train_batch_size", str(batch_size),
    "--gradient_accumulation_steps", str(gradient_accumulation_steps),
    "--learning_rate", str(learning_rate),
    "--max_train_steps", str(num_train_steps),
    "--validation_prompt", validation_prompt,
    "--num_validation_images", str(num_validation_images),
    "--validation_steps", str(validation_steps),
    "--checkpointing_steps", str(validation_steps),  # Save checkpoints at validation steps
    "--checkpoints_total_limit", "3"  # Keep only last 3 checkpoints to save space
]

# Add optimization flags
if use_xformers:
    command.append("--enable_xformers_memory_efficient_attention")
if use_8bit_adam:
    command.append("--use_8bit_adam")
if gradient_checkpointing:
    command.append("--gradient_checkpointing")
if train_text_encoder:
    command.append("--train_text_encoder")
if use_fp16:
    command.append("--mixed_precision=fp16")

# Add hub settings if enabled
if push_to_hub:
    command.extend([
        "--push_to_hub",
        "--hub_model_id", hub_model_id
    ])

print("Training command:")
print(" ".join(command))

# Run training
try:
    result = subprocess.run(command, check=True)
    print("\nTraining completed successfully!")
except subprocess.CalledProcessError as e:
    print(f"\nTraining failed with error code {e.returncode}")
    print("Check the error messages above for details.")

# %% [markdown]
# ## Inference with Trained Model
#
# After training completes, you can generate images using your trained LoRA adapter. This code is memory-optimized for Colab's T4 GPU:

# %% [code]
# @title Generate Images with Trained Model
from diffusers import StableDiffusionXLPipeline, UNet2DConditionModel
from safetensors.torch import load_file
import torch
import gc

# Clear GPU memory
torch.cuda.empty_cache()
gc.collect()

# Load the base model
print("Loading base SDXL model...")
base_model = StableDiffusionXLPipeline.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    variant="fp16",
    use_safetensors=True
).to("cuda")

# Load the LoRA weights
print("Loading LoRA weights...")
lora_path = f"{output_dir}/pytorch_lora_weights.safetensors"
if not os.path.exists(lora_path):
    # Try alternative path
    lora_path = f"{output_dir}/checkpoint-{num_train_steps}/pytorch_lora_weights.safetensors"

if os.path.exists(lora_path):
    base_model.load_lora_weights(lora_path)
    print(f"Successfully loaded LoRA weights from: {lora_path}")
else:
    print(f"Warning: LoRA weights not found at {lora_path}")
    print("Using base model without LoRA weights")

# Enable memory optimizations
if use_xformers:
    base_model.enable_xformers_memory_efficient_attention()

# Generate images
print("\nGenerating images...")
generator = torch.Generator(device="cuda").manual_seed(42)

# Create multiple prompts to show model capability
prompts = [
    validation_prompt,
    f"a professional portrait of a {instance_prompt.split()[-1]}",
    f"a {instance_prompt.split()[-1]} in business attire",
    f"headshot of a {instance_prompt.split()[-1]} professional"
]

images = []
for i, prompt in enumerate(prompts):
    print(f"Generating image {i+1}/{len(prompts)} with prompt: '{prompt}'")
    image = base_model(
        prompt,
        num_inference_steps=30,
        guidance_scale=7.5,
        generator=generator,
        height=resolution,
        width=resolution
    ).images[0]
    images.append(image)
    
    # Save individual image
    image_path = f"{output_dir}/generated_{i}.png"
    image.save(image_path)
    print(f"Saved image to: {image_path}")

# Display images
print("\nGenerated images:")
for i, img in enumerate(images):
    display(img)
    print(f"Image {i+1}: {prompts[i]}")

# Save all images in a grid
from PIL import Image
import math

# Create grid
cols = 2
rows = math.ceil(len(images) / cols)
grid_width = resolution * cols
grid_height = resolution * rows
grid = Image.new('RGB', (grid_width, grid_height))

for i, img in enumerate(images):
    x = (i % cols) * resolution
    y = (i // cols) * resolution
    grid.paste(img, (x, y))

grid_path = f"{output_dir}/generated_grid.png"
grid.save(grid_path)
print(f"\nSaved grid image to: {grid_path}")
display(grid)

# %% [markdown]
# ## Download Your Trained Model
#
# Your trained LoRA weights are saved in the output directory. You can download them:

# %% [code]
# @title Download Trained Model
from google.colab import files
import zipfile
import os

# Create zip file of the output directory
zip_path = f"/content/{project_name}.zip"
with zipfile.ZipFile(zip_path, 'w') as zipf:
    for root, dirs, files in os.walk(output_dir):
        for file in files:
            file_path = os.path.join(root, file)
            arcname = os.path.relpath(file_path, output_dir)
            zipf.write(file_path, arcname)

print(f"Created zip file: {zip_path}")
print("Click the link below to download your trained model:")

# Provide download link
from IPython.display import FileLink
FileLink(zip_path)

# %% [markdown]
# ## Cleanup (Optional)
#
# Free up disk space by removing temporary files:

# %% [code]
# @title Cleanup (Optional)
# Clear GPU memory
torch.cuda.empty_cache()
gc.collect()

# Remove large temporary files (optional)
# !rm -rf /content/diffusers
print("Cleanup completed!")